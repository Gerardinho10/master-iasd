{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression on Criteo dataset with Spark MLlib\n",
    "\n",
    "In this exercise, we will build a logistic regression model on the Criteo dataset using Spark MLlib.\n",
    "\n",
    "First a few imports to help you find what is needed for the exercise. Reminder, you can easily access the documentation of a methode by typing method? in a notebook cell.\n",
    "\n",
    "Then we create a spark application running locally on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as st\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"criteo-lr\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.ui.port\", \"0\") \\\n",
    "    .getOrCreate()\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Load the data as a Spark DataFrame\n",
    "\n",
    "Use the function ss.read.csv to read the train.txt file.  \n",
    "Don't forget to specify the separator is \"\\t\", there is no header.  \n",
    "For the schema, you can either infer the schema automatically on a subset of the dataset using inferSchema and samplingRatio parameters or build the schema using st.StructType, st.StructField, st.IntegerType and st.StringType.\n",
    "\n",
    "For faster execution when building the code for the next questions, sample the dataset at 1% and persist the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Simple stats\n",
    "\n",
    "Using the Spark dataframe API, compute the number of training examples, the number of positive and negative examples and the average probability of the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Train Test Split\n",
    "\n",
    "Split the dataframe in a train and test dataframes using 80% for train and 20% for test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Feature Hashing\n",
    "\n",
    "For now we will restrict ourselves to the categorical features.  \n",
    "Hash all categorical features on a 2^16 space using FeatureHasher.\n",
    "\n",
    "Observe a few lines of the hashed features. How are the features represented after hashing ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Fit a logistic regression model\n",
    "\n",
    "Create and fit a logistic regression model using LogisicticRegression.  \n",
    "Use L2 regulization.  \n",
    "Limit the maximum number of iterations of the optimization algorithm to something small for fast iteration.\n",
    "\n",
    "Print the model intercept and compute the sigmoid of the intercept (using scipy expit for example). What do you notice ?\n",
    "\n",
    "Using matplotlib, print an histogram of the model coefficients. What do you notice ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6: Set up a pipeline\n",
    "\n",
    "Your model consist of two steps: feature hashing and logistic regression. Assemble both steps into a Pipeline to ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7: Model evaluation\n",
    "\n",
    "Compute your model prediction on the test dataframe.  \n",
    "Using BinaryClassificationEvaluator, compute the areaUnderROC metric on the test dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPT] Q8: Bucketization of integer features\n",
    "\n",
    "Using QuantileDiscretizer, bucketize to 20 buckets every integer features. Then add those features in the hasher and relaunch training and evalution. By how much the AUC metric improves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to close the spark application at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
