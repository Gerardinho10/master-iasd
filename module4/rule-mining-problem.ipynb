{"cells":[{"cell_type":"markdown","source":["# Practical Session 3: Association rule mining\n\nIn this session, we will build a first set of algorithms to infer rules from a dataset.  This is known as association rule mining (e.g. people who buy potates and bread are likely to be building a burger and therefore interested in salad and steaks)"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%matplotlib inline"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Downloading and unzipping the data (MovieLens)"],"metadata":{}},{"cell_type":"code","source":["import urllib\nimport zipfile\n\nurl = 'http://files.grouplens.org/datasets/movielens/ml-20m.zip'\nfilehandle, _ = urllib.request.urlretrieve(url, '/tmp/data.zip')\nzip_file_object = zipfile.ZipFile(filehandle, 'r')\nzip_file_object.namelist()\nzip_file_object.extractall()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Reading data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Dataset\") \\\n    .getOrCreate()\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["movies_path = \"file:///databricks/driver/ml-20m/movies.csv\"\nratings_path = \"file:///databricks/driver/ml-20m/ratings.csv\""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["We read the csv files using [`spark.read`](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)"],"metadata":{}},{"cell_type":"code","source":["movies_df = spark.read.options(header=True).csv(movies_path)\n# TASK 1: explain what the filter below does.  Why did we not use sample instead?\nratings_df = spark.read.options(header=True).csv(ratings_path).filter(sf.expr('PMOD(HASH(userId),10)')==0)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["We cache the read dataframes to avoid reloading them in subsequent computation."],"metadata":{}},{"cell_type":"code","source":["movies_df.cache()\nratings_df.cache()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["We then print a few rows from each dataframe."],"metadata":{}},{"cell_type":"code","source":["movies_df.show(5)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["ratings_df.show(5)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["ratings_df.select('movieId').distinct().count()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["ratings_df.select('userId').distinct().count()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql import functions as sf\nimport pyspark"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# TASK 2: filter ratings to keep only the latest 100 ratings per user\n# Hint: create a new column called tm_rank that sorts the ratings per timestamp and per hash of movie id.\n#       use sf.rank(), sf.struct(), sf.hash() to create this column, then filter() to filter the data\n#       do not forget to drop this new column once you are done.\nlim_ratings_df = ..."],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["lim_ratings_df.show(5)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# sanity check: this should return a min of 20 and a max of 100\nlim_ratings_df\\\n    .groupby('userId')\\\n    .agg(sf.count('*').alias('num_ratings'))\\\n    .agg(sf.min('num_ratings'), sf.max('num_ratings'))\\\n    .toPandas()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### 1. Naive approach: Find recurring pairs & triplets.\nThis approach is simple and not efficient but gives you a baseline and intuition for the next steps."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# TASK 3: find recurring pairs with a naive approach, then show the top 25 results\n# Remember to use the title field to make the results interpretable\n# Also, make sure to work with lim_ratings_df, not ratings_df!\n\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["all_movie_pairs_df.count()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# TASK 4: find recurring triplets and show the top 25 results."],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### 2. Second approach: A priori  \nImplement your own version of A priori.  You may use resources from the web.\nhttps://fr.wikipedia.org/wiki/Algorithme_APriori"],"metadata":{}},{"cell_type":"code","source":["# TASK 5: implement the a priori approach to find recurring pairs and triplets more efficiently"],"metadata":{},"outputs":[],"execution_count":29}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3","nbconvert_exporter":"python","file_extension":".py"},"name":"rule-mining","notebookId":258734209120091},"nbformat":4,"nbformat_minor":0}
